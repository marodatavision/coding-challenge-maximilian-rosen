import sys
from dotenv import load_dotenv

from app.config import settings
from app.services.document_loader import load_and_split
from app.services.embedding_service import embed_chunks
from app.services.query_service import QueryService
from app.adapters.vectorstore.faiss_retriever import FAISSRetriever
from app.adapters.llm.openai_client import OpenAIClient
from app.core.models.query import Query


def main():
    load_dotenv()

    print(f"ğŸ” Lade und verarbeite Dokumente ...")
    chunks = load_and_split()

    if not chunks:
        print("â— Es wurden keine Dokumente gefunden oder verarbeitet.")
        sys.exit(1)

    print(f"ğŸ“¦ Erstelle Embeddings ...")
    #embedded_chunks = embed_chunks(chunks)

    print(f"âš™ï¸ Initialisiere Komponenten ...")
    retriever = FAISSRetriever(chunks=chunks)
    llm = OpenAIClient()
    query_service = QueryService(retriever, llm)

    print("\nğŸŸ¢ Starte Chat. Gib 'exit' ein, um zu beenden.\n")

    while True:
        user_input = input("ğŸ’¬ Deine Frage: ").strip()
        if user_input.lower() in {"exit", "quit"}:
            print("ğŸ‘‹ Bis zum nÃ¤chsten Mal!")
            break

        if not user_input:
            continue

        query = Query(text=user_input)
        answer = query_service.run(query)

        print("\nâœ… Antwort:\n")
        print(answer.text)

        if answer.sources:
            print("\nğŸ”— Quellen:")
            for src in answer.sources:
                print(f"- {src}")
        print("\n" + "-" * 60 + "\n")


if __name__ == "__main__":
    main()
